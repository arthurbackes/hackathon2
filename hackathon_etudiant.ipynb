{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f5f3285-f41f-4595-981e-9fb69bf0806e",
   "metadata": {},
   "source": [
    "# **[LEPL1109] - STATISTICS AND DATA SCIENCES**\n",
    "## **Hackathon 02 - Classifying and clustering images**\n",
    "\\\n",
    "Prof. D. Hainaut\\\n",
    "Prod. L. Jacques\\\n",
    "\\\n",
    "\\\n",
    "Adrien Banse (adrien.banse@uclouvain.be)\\\n",
    "Jana Jovcheva (jana.jovcheva@uclouvain.be)\\\n",
    "François Lessage (francois.lessage@uclouvain.be)\\\n",
    "Valentin de Bassompierre (valentin.debassompierre@uclouvain.be)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e30476-0d4b-4deb-8eb5-7b54af9dc136",
   "metadata": {},
   "source": [
    "![alt text](figures/hyperspectral.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578cefc-e90d-4120-a8d9-f40ea76b0014",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<b>[IMPORTANT] Read all the documentation.</b>  <br>\n",
    "    Make sure that you read the whole notebook before completing it. The teaching team spent some time to introduce hyperlinks to useful external concepts (such as the description of a python object of function). Use them. If some concepts are unclear, you are also supposed to document by yourself while preparing the hackathon.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395a8f8-411f-4091-8a28-769fa5f8a10f",
   "metadata": {},
   "source": [
    "# **Guidelines and Deliverables**\n",
    "\n",
    "*   This hackathon is due on the **19th of December at 23:59**. No extension will be given. \n",
    "*   This year, there is only one hackathon, which is therefore more substantial, but you have more time to complete it.\n",
    "*   Copying code or answers from other groups (or from the internet) is strictly forbidden. <b>Each source of inspiration (stack overflow, git, other groups, ChatGPT...) must be clearly indicated!</b>\n",
    "*  This notebook (with the \"ipynb\" extension) file and all other files that are necessary to run your code must be delivered on <b>Moodle</b>.\n",
    "*  Only this notebook will be graded.\n",
    "    * 2/20 if the codes executes well.\n",
    "    * 18/20 for the answers to the questions. <br>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[NOTE] Use of generative AI</b>  <br>\n",
    "    As mentioned above, plagiarism is forbidden. However, we cannot forbid you to use artificial intelligence BUT we remind you that the aim of this project is to learn on your own and with the help of the course material. Finally, we remind you that for the same question, artificial intelligence presents similar solutions, which could be perceived as a form of plagiarism. Be also aware that the teaching team checked the answers AI could provide to most of the hackathon questions; the results were disappointing and they do not reach the required level to succeed this hackathon.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c0552e-3383-4477-9d2f-43b345bc0d28",
   "metadata": {},
   "source": [
    "# **Context & Objective**\n",
    "Image analysis is widely used in areas ranging from environmental monitoring to computer vision. However, it is crucial to use the right tools to tackle different problems. Clustering and classification techniques such as **k-nearest neighbors (k-NN)** and **k-means** are foundational tools in data science, but **their effectiveness varies depending on the nature of the data**.\n",
    "\n",
    "In this hackathon, you will be asked to explore the strengths and limitations of these methods are **3 distinct types of images dataset**: \n",
    "\n",
    "1. **Satellite RGB/RGBA images**: often used for land cover classification\n",
    "2. **Dogs and cats RGB/RGBA images**: a classic benchmark in computer vision for object recognition and classification\n",
    "3. **Hyperspectral satellite images**: high-dimensional data capturing information across multiple spectral bands, widely used in precision agriculture\n",
    "\n",
    "Each dataset presents unique characteristics, such as dimensionality and feature correlation.\n",
    "\n",
    "In this hackathon, your mission is to: \n",
    "- Apply k-NN and k-means to the first two datasets, and k-NN to the third dataset. \n",
    "- Evaluate the approaches for each dataset.\n",
    "- Analyze why certain methods work better for certain types of images, linking your findings to the inherent properties of the datasets.\n",
    "\n",
    "## **Notebook structure**\n",
    "\n",
    "* PART I - Satellite images\n",
    "   - I.1 - Pre-processing\n",
    "   - I.2 - k-NN classification\n",
    "   - I.3 - Classification using k-means\n",
    "   - I.4 - PCA\n",
    "    <br>\n",
    "* PART II - Dogs and Cats\n",
    "   - II.1 - Pre-processing\n",
    "   - II.2 - k-NN classification\n",
    "   - II.3 - PCA\n",
    "   <br>\n",
    "* PART III - Hyperspectral images\n",
    "   - III.1 - Pre-processing\n",
    "   - III.2 - k-NN classification\n",
    "   - III.2 - k-NN classification with block-based sampling\n",
    "   - 3.3 - KNN regressor\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[NOTE] Divide to conquer</b>  <br>\n",
    "This hackathon was designed so that it is possible to work on each dataset independently. <b>Take advantage of this aspect to divide the work between all team members!</b>\n",
    "However, keep in mind that all the material in this hackathon will be relevant for the final exam, so every team member should make sure they understand all parts of the project.\n",
    "</div>\n",
    "\n",
    "## **External libraries**\n",
    "\n",
    "To support you in completing this hackathon, we’ve listed some external libraries and functions that the teaching staff used to solve the tasks. Think of this as a helpful suggestion: you’re free to use other imports if you prefer. Just keep in mind that others have already faced similar challenges — so don’t spend too much time reinventing the wheel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69f0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "from skimage import io, transform\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import RocCurveDisplay, auc, roc_curve\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import mode\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84284923-00ba-48ed-850c-1f88077ab5b0",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART I - Satellite images</b> </font> <br><br>\n",
    "\n",
    "In this part, you have a dataset containing satellite images, each of which figures one of four possible land types (green area, cloudy, water or desert). We refer to these land types as the \"labels\" of the images. The task at hand is to predict the label of each image, i.e. to which land type it corresponds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7cc298-ff96-4291-ad31-929279696b04",
   "metadata": {},
   "source": [
    "![alt text](datasets/satellite/cloudy/train_12.jpg)\n",
    "![alt text](datasets/satellite/desert/desert(1).jpg)\n",
    "![alt text](datasets/satellite/cloudy/train_14.jpg)\n",
    "![alt text](datasets/satellite/desert/desert(2).jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb606b4-5dac-4182-b683-6c3870a256e0",
   "metadata": {},
   "source": [
    "# **Part I.1 - Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131999d-9672-43ac-bbe5-aafd02f00930",
   "metadata": {},
   "source": [
    "In this first part, you will be asked to pre-process the data. For this you are going to use the tools of the pandas toolbox and you are asked to produce a `pd.DataFrame` (read the doc [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) to understand this type of object) whose rows correspond to images, and whose columns contain \n",
    "- Either the [RGBA](https://en.wikipedia.org/wiki/RGBA_color_model) value of a pixel — the majority of columns\n",
    "- Or the label of the image (cloud, desert, ...) — one such column\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[NOTE] Pre-processing</b>  <br>\n",
    "You should be careful about the following characteristics of the dataset: \n",
    "    \n",
    "- The images are not necessarily of the same resolution\n",
    "- Some images only contain [RGB](https://en.wikipedia.org/wiki/RGB_color_model) values, whereas others contain [RGBA](https://en.wikipedia.org/wiki/RGBA_color_model) values.\n",
    "- Remove all images that contain `NaN` values.\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e3682-2220-4da9-af4d-2e668643ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"datasets/satellite\"      \n",
    "LABEL_TO_IDX = {\"cloudy\": 0, \"desert\": 1, \"green_area\": 2, \"water\": 3}\n",
    "\n",
    "# TO DO: fill the dataframe with the satellite images.\n",
    "df = pd.DataFrame(...)\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4970759-54dd-41e9-8ce9-08d355d613a6",
   "metadata": {},
   "source": [
    "# **Part I.2 - k-NN classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b371b5dc-dbd7-4b14-9c0f-7236b2433c66",
   "metadata": {},
   "source": [
    "You now have a dataset containing images with some pixels in [RGBA](https://en.wikipedia.org/wiki/RGBA_color_model) format, and each image is associated with a label (green area, cloudy, water and desert) indicating the class to which it belongs. The label is the category or class you will try to predict. The features (pixels) are the inputs used to learn how to predict the label. In this second part, you are asked to train a k-NN classifier on your dataset. In order to do this, first **split your dataset** in a training set containing 75% of the images, and a testing set containing 25% of it. Also, **scale** your dataset. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[NOTE] Random seed</b>  <br>\n",
    "    \n",
    "In order to ensure the reproducibility of your results across different runs, set a [random seed](https://en.wikipedia.org/wiki/Random_seed) . \n",
    "</b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[NOTE] Standardization</b>  <br>\n",
    "\n",
    "In real-world datasets, different features are not on the same numerical scale: one variable may be expressed in meters, another in seconds, another in gray intensity, another in numbers, etc. If we directly introduce these raw features into algorithms such as k-NN or K-Means, which are based on distances, features with larger numerical ranges dominate the distance calculation, even though they are not more informative. Standardization solves this problem by placing all features on an equal footing, typically with a mean of 0 and a variance of 1.\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{N \\times d}$ be your dataset, with $N$ datapoints and $d$ features (columns), it can be written as $X = (x^1 \\dots x^d)$, with $x^i \\in \\mathbb{R}^N$. \n",
    "\n",
    "A column $x^i$ can be standardized if it is replaced by $z^i = (x^i - \\mu^i) / \\sigma^i$, with $\\mu^i = \\frac{1}{N}\\sum_{j = 1}^N x^i_j$ and $\\sigma^i = \\sqrt{\\frac{1}{N} \\sum_{j = 1}^N (x_j^i - \\mu^i)^2}$. \n",
    "\n",
    "A dataset is standardized if all its columns are standardized. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282793f9-f21a-4d30-b876-53663c2d4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: split your dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2dbb83-8858-4dc9-9cb5-131c899e79d3",
   "metadata": {},
   "source": [
    "You will have to select a parameter `k`, the number of nearest neighbours. **Select the best parameter `k` between the following values: `[5, 10, 50, 100, 150, 200]`.**\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[QUESTION 1] Parameter selection</b>  <br>\n",
    "    \n",
    "- In order to choose the best <code>k</code> parameter, should you use the entire dataset or only the available data?\n",
    "- How did you choose the best <code>k</code> parameter (which method did you use)?\n",
    "\n",
    "Explain and justify. (Expected answer length: &le; 9 lines).\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>[QUESTION 1] Answer</b>  <br>\n",
    "**Insert your answer here.**\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831ce46-8a1a-41f1-a72a-8e8d5a084d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: find the best 'k' parameter\n",
    "k = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2327782c-221f-4a52-8090-f6a7d003e4a7",
   "metadata": {},
   "source": [
    "Given the best `k`, **train a k-NN classifier** on your dataset. As seen in the course, use the `euclidean` distance between your points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb6bea4-8a10-455e-afbe-577002e76825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: instantiate a k-NN classifier and train it on the satellite dataset\n",
    "model = KNeighborsClassifier(...)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef161b-22b1-44b6-bb61-dd82f7cebcb6",
   "metadata": {},
   "source": [
    "Now that you have a model, **evaluate it** in the following way: \n",
    "- Compute the accuracy of your model on your testing set.\n",
    "- Plot the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6287b-5de6-4270-aa26-3420be939e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: compute the accuracy on the testing set\n",
    "accuracy = ...\n",
    "print(\"The accuracy of the model is \", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc542ed-fa96-4ecd-980b-fe995af527d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: compute and plot the confusion matrix for the test set\n",
    "cm = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded0e7b-00f1-4c9b-84c5-d0e7619b8447",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[QUESTION 2] Model evaluation</b>  <br>\n",
    "    \n",
    "- What can you say about the performance of your model?\n",
    "- Is k-NN a good algorithm to classify these images?\n",
    "- Why are some of the labels better classified than others? Give an intuitive explanation, and link it to the accuracy and confusion matrix.\n",
    "    \n",
    "(Expected answer length: &le; 9 lines).\n",
    "</b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>[QUESTION 2] Answer</b>  <br>\n",
    "**Insert your answer here.**\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e703b05-95dc-45cc-a1cd-4be04f64ddf1",
   "metadata": {},
   "source": [
    "We also ask you to produce a ROC curve to evaluate your classifier. In the course, we've seen how to do this for binary classification, i.e., when there are only two classes to differentiate. Here, there are more than two classes, and the method used must be adapted. In practice, there are several ways to do this, but here we ask you to use a One-vs-Rest strategy as explained in the link [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html). The idea is to compare each class against all the other classes considered as one, thus reverting to binary classification. Then, to evaluate the classifier as a whole, the results for each class can be averaged (see [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html)).\n",
    "\n",
    "We ask you to one **produce a multi-class ROC plot** containing: \n",
    "- A One-vs-Rest (OvR) ROC curve, for each class (4 curves)\n",
    "- A ROC curve using the OvR micro-average (1 curve)\n",
    "- A ROC curve using the OvR macro-average (1 curve)\n",
    "\n",
    "On the plot, indicate the value of the area under the curve (AUC) for each curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f4692-2115-4d0b-91df-248e6050effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: plot the multi-class ROC curves (in a single figure)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e3bb0-275a-4fce-acb3-b8480d719aee",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[QUESTION 3] ROC Curve</b>  <br>\n",
    "\n",
    "- Can you make the link between these ROC curves and the confusion matrix above?\n",
    "- What is the difference between micro- and macro-average in this context?\n",
    "- What information can you infer from the fact that both the micro- and macro-average curves are close to the upper left part of the figure?\n",
    "\n",
    "(Expected answer length: &le; 12 lines).\n",
    "</b>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>[QUESTION 3] Answer</b>  <br>\n",
    "**Insert your answer here.**\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1164cdf-d12f-483c-93bc-8198271cf51a",
   "metadata": {},
   "source": [
    "# **Part I.3 - Classification using k-means**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35772b09-79e7-429a-8ae0-0388f762342f",
   "metadata": {},
   "source": [
    "In this third part, you are asked to cluster your dataset using the k-means algorithm. Even though K-Means is an unsupervised algorithm, it can still be informative and useful in a supervised learning setting. The key idea is that unsupervised structure can reveal information that helps classification. To do that, first **scale your dataset**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e0846c-5244-46c1-9775-da21d5d54c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Standardize your dataset\n",
    "x_train_scaled = ...\n",
    "x_test_scaled = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4c74e2-6513-457e-b98f-12ac9563e6f1",
   "metadata": {},
   "source": [
    "Now, **train a k-means clustering model**. Train it with `20` different centroid seeds. Again, make sure to set a [random seed](https://en.wikipedia.org/wiki/Random_seed) to ensure reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972961a-4b38-4ab9-a39f-5da5b2db6c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: instantiate a k-means clustering model and train it on the satellite dataset\n",
    "kmeans = KMeans(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b53cad-0719-4a23-8e8d-ac6f2b14e6c6",
   "metadata": {},
   "source": [
    "In order to transform a clustering model into a classifier, we need to assign each cluster to a label. To do that, use your training set and perform a **majority vote**. That means, assign each cluster to the label that is the most present in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c004ff-c575-4a18-9e2d-382acb0a74a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: assign a label to each cluster\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e676d06-5607-49ca-b3dc-52cfcc158be3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[QUESTION 4] Supervised or unsupervised?</b>  <br>\n",
    "Is this way of using k-means supervised or unsupervised? Justify. (Expected answer length: &le; 3 lines).\n",
    "</b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>[QUESTION 4] Answer</b>  <br> \n",
    "**Insert your answer here.**\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31e9972-2c30-478a-93f4-573ca48206fb",
   "metadata": {},
   "source": [
    "Now that you have a model, **evaluate it** in the following way: \n",
    "- Compute the accuracy of your model on your testing set using the assignment computed at the last step. \n",
    "- Plot the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5977d63-9d36-42ad-8539-5f202560b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: compute the accuracy on the testing set\n",
    "accuracy = ...\n",
    "print(\"The accuracy of the model is \", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f94d27-2d59-463c-9e4e-78906e0a8395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: compute and plot the confusion matrix for the test set\n",
    "cm = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4682484-196a-47c5-88cd-a4de5ad8e589",
   "metadata": {},
   "source": [
    "# **Part I.4 - PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db37109b-7758-48c8-b655-24e5e39b7dd4",
   "metadata": {},
   "source": [
    "In order to interpret the results above, we will try to visualize the dataset. **Reduce the dimensionality** of the entire dataset to 3 dimensions using **Principal Component Analysis (PCA)**. You can restrict your analysis to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7284004f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a400b-4334-47e8-9ca8-637a14c250fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: use PCA on the dataset\n",
    "x_train_reduced = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9c690-fdce-45c6-a779-e3bea238d018",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[QUESTION 5] PCA</b>  <br>\n",
    "    \n",
    "- Why is it important to standardize the dataset before applying a PCA?\n",
    "- Should you compute the eigenvectors of the **covariance** or the **correlation**?\n",
    "    \n",
    "In your answer, include an explanation in the context of this very dataset, i.e. $(R, G, B, A)$ images. (Expected answer length: &le; 7 lines).\n",
    "\n",
    "</b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>[QUESTION 5] Answer</b>  <br>\n",
    "**Insert your answer here.**\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a05a2d-ec8c-42f4-a622-6b0f7ff65784",
   "metadata": {},
   "source": [
    "**Make a 3D plot** of the reduced dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe121f-dd7d-4dc8-8302-c8e51d4e1fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TO DO: Make a 3D plot of the reduced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2661150-4277-49ff-a2df-c01574cd8623",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[QUESTION 6] Interpretation of k-means results</b>  <br>\n",
    "\n",
    "- Interpret the results of the k-means clustering algorithm. Make the link between these results and the 3D plot above. \n",
    "- Is it a good idea to use k-means here?\n",
    "\n",
    "(Expected answer length: &le; 9 lines).\n",
    "</b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>[QUESTION 6] Answer</b>  <br>\n",
    "**Insert your answer here.**\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733fac26-e574-4ed3-985d-848f1c1a464d",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART II - Dogs and Cats</b> </font> <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ccbe2-c30a-4b93-b665-53a9c13f6057",
   "metadata": {},
   "source": [
    "<img src=\"datasets/cats_and_dogs/dogs/dog_0.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/> <img src=\"datasets/cats_and_dogs/cats/cat_3.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c55a7-6822-42e6-9331-2b0dca482cc0",
   "metadata": {},
   "source": [
    "In this second part, we will try to use similar methods for the classification of **pictures of dogs and cats**. In particular, we will\n",
    "- Pre-process the dataset\n",
    "- Train a k-NN classifier, and evaluate it\n",
    "- Use PCA to visualize the dataset\n",
    "\n",
    "Then, we will compare how these methods perform compared to the first dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc2b18-a08c-4860-93f9-972c893fbbfd",
   "metadata": {},
   "source": [
    "# **Part II.1 - Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576dad5-8467-4285-8657-05646ae5c9d4",
   "metadata": {},
   "source": [
    "**Pre-process the dataset** in the same fashion as the first dataset. That is, produce a `pd.DataFrame` (see [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)) whose rows correspond to images, and whose columns contain either the RGBA value of a pixel or the label of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644dcc28-6030-452a-bb3b-827a048dffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: fill the dataframe with the Cats&Dogs images\n",
    "DATA_PATH = \"datasets/cats_and_dogs\"\n",
    "LABEL_TO_IDX = {\"cats\": 0, \"dogs\": 1}\n",
    "\n",
    "df = pd.DataFrame(...)\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c60417e-a4cc-4305-b58a-f224c607b42a",
   "metadata": {},
   "source": [
    "# **Part II.2 - k-NN classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b5bbaf-bac3-4500-aa83-702f089c1b14",
   "metadata": {},
   "source": [
    "**Train a k-NN classifier** in the same fashion as for the first dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080a38e-1c8d-4504-b5d0-55849e29461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: train a k-NN classifier on this dataset\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba6046-d53c-40d6-8ee2-146710145b8f",
   "metadata": {},
   "source": [
    "**Evaluate your k-NN classifier** by \n",
    "- Computing the accuracy\n",
    "- Plotting the Confusion Matrix\n",
    "- Plotting the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb36f1c-5ddc-4c83-bc88-3ec1a5c27474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: compute the accuracy on the test set\n",
    "accuracy = ...\n",
    "print(\"The accuracy of the model is \", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce21a9-cbba-4a44-a5b4-853d949a1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: compute and plot the confusion matrix for the test set\n",
    "cm = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d90a3-eb36-4e07-91be-daa4b390f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: plot the ROC curve of your k-NN classifier\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c85c5e-66e9-4dd6-b600-30d7a6c664bb",
   "metadata": {},
   "source": [
    "# **Part II.3 - PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51cbc4e-ee23-48c9-8a32-45d9fd6571c2",
   "metadata": {},
   "source": [
    "In order to have further insight about this dataset, **reduce the dimension** of the dataset to 3 dimensions using **Principal Component Analysis (PCA)**. Again, your analysis can be restricted to the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808aba2c-adf0-4392-95a0-fd0e654a6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: use PCA on the dataset\n",
    "x_train_reduced = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ed361-0bd2-485b-8f20-7d647fa87496",
   "metadata": {},
   "source": [
    "Make the same 3D plot as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec82815-ae30-47a2-ae12-9769515989dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: make a 3D plot of the reduced dataset.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db0e02-6bb9-4d3d-9ca3-0a9a9be905bc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[QUESTION 7] Analysis of the results on this dataset</b>  <br>\n",
    "Is there a difference of performance between the k-NN trained on the first dataset (satellite image) and the second dataset (cats and dogs)? If so, highlight the main visual difference between the appearance of the images of the first and second datasets that could explain such a difference in performance and the difficulty level of the considered classification problem. Be precise, and make the link between your explanation and the accuracy, confusion matrix, ROC curve and PCA plot. (Expected answer length: &le; 9 lines).\n",
    "</b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>[QUESTION 7] Answer</b>  <br>\n",
    "**Insert your answer here.**\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223c083-f840-4fe0-83c0-6c963102bd52",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART III - Hyperspectral images</b> </font> <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7096f-f214-4c2d-a64d-a403c5f792bb",
   "metadata": {},
   "source": [
    "<img src=\"figures/salinas_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <img src=\"figures/salinas_2.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8472144f-ae07-4841-8210-2906a801f33f",
   "metadata": {},
   "source": [
    "We will now train and test our model on a hyperspectral image collected by the AVIRIS sensor over Salinas Valley, California.\n",
    "This dataset is a classic benchmark in remote sensing, containing agricultural fields with 16 labeled classes, including vegetables, bare soil, and vineyards. The raw image has 224 spectral bands, of which 20 noisy water-absorption bands have been removed, leaving **204 effective bands**. The final image has a spatial resolution of **512 lines × 217 samples**.\n",
    "\n",
    "Unlike ordinary RGB images, which record only three broad color channels (red, green, blue), hyperspectral images capture **hundreds of narrow spectral bands**, each corresponding to a specific wavelength. This fine spectral resolution reveals subtle material and chemical differences that ordinary color cannot.\n",
    "\n",
    "To visualize this structure, consider the hyperspectral cube below.\n",
    "Each pixel $(x,y)$ in the spatial plane contains an entire spectrum across wavelengths $\\lambda$. \n",
    "The curve on the right illustrates this spectral signature, which quantifies how that pixel reflects or absorbs light across the electromagnetic spectrum.\n",
    "\n",
    "We invite you to spend a few minutes to learn more about the [Salinas dataset](https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15704d3",
   "metadata": {},
   "source": [
    "<img src=\"figures/HSI_diagram.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969810c2-bb32-4762-817a-d258123897ef",
   "metadata": {},
   "source": [
    "# **Part III.1 - Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ced7d-dd1b-463f-8c44-3e6206f6ec51",
   "metadata": {},
   "source": [
    "With RGB/RGBA images, we had multiple separate images, each with a single label (e.g., \"dog\" or \"cat\").\n",
    "\n",
    "With the hyperspectral image, you have one large image: **you’ll crop it into smaller spatial patches** (e.g. groups of pixels), **split the collection of these patches into training and testing sets**, and **classify each patch based on its spectral signature**. The key difference is that you’re working with spatial regions of a single image rather than independent images.\n",
    "\n",
    "For the hyperspectral image, you are given:\n",
    "\n",
    "- A single large image (`img`) with height `H`, width `W`, and `B` spectral bands.\n",
    "- A corresponding ground truth label map (`gt`) of the same height and width, where each pixel’s value indicates its class (e.g., 1 for forest, 2 for water, etc.), and 0 means background (to be ignored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b34fc6e-6c86-4a92-832b-ecc988c856e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "# Load data\n",
    "data = loadmat('datasets/hyperspectral/hyperspectral.mat')\n",
    "gt_data = loadmat('datasets/hyperspectral/hyperspectral_gt.mat')\n",
    "\n",
    "img = data['salinas_corrected']  # shape: H x W x B\n",
    "gt = gt_data['salinas_gt']       # shape: H x W, 0 = background\n",
    "\n",
    "H, W, B = img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f53349e-48a8-47dd-945a-888b5333eac7",
   "metadata": {},
   "source": [
    "Your tasks are the following: \n",
    "\n",
    "1. Flatten the image cube into a 2D array of pixels. After flattening, each row of the array corresponds to one pixel, and each column represents a wavelength band. In other words, every row gives the spectral signature (intensity vs. wavelength) for one spatial location $(x,y)$ in the image cube. The above diagram of a hyperspectral cube may be helpful.\n",
    "\n",
    "2. Flatten the label image in the same way so that each label matches the corresponding pixel.\n",
    "\n",
    "3. Remove background pixels (where the label value is 0), keeping only labeled pixels.\n",
    "\n",
    "4. Shuffle the remaining labeled pixels and split them into training (75%) and test (25%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9dd2a7-d949-4bf6-ac47-b42cd99fcaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: divide your dataset into training and testing sets, according to the methodology described above.\n",
    "X_train, y_train = ...\n",
    "X_test, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4949cb-ecab-4894-9a75-c4807d891171",
   "metadata": {},
   "source": [
    "# **Part III.2 - k-NN classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee8f56-fe67-445e-b2fd-f8a870d0c540",
   "metadata": {},
   "source": [
    "**Train a k-NN classifier** with `k = 5` and the `euclidean` distance. Before training it, you should **normalize** the rows of your dataset.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[NOTE] Normalization</b>  <br>\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{N \\times d}$ be your dataset, with $N$ data points and $d$ features (columns).  It can be written as $X = (x_1, \\dots ,x_N)^\\top$ where $x_i \\in \\mathbb{R}^d$.\n",
    "\n",
    "A row $x_i$ can be normalized if it is replaced by $z_i = x_i / \\|x_i\\|$.  \n",
    "A dataset is normalized if all its rows are normalized. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1ed7d-07c7-45b7-b7c9-a6a34e7fa3f0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[QUESTION 8] Normalization</b>  <br>\n",
    "In this example, when using the Euclidean distance, why is normalization important?  Explain your reasoning in terms of the spectral features of the pixels (their \"shape\" and \"intensity\").\n",
    "\n",
    "<i>Hint:</i> The Euclidean distance is sensitive to the <b>magnitude</b> of the vectors. \n",
    "\n",
    "<i>Hint:</i> You may find it helpful to have another look at the spectral signature subplot of the hyperspectral cube diagram.\n",
    "\n",
    "(Expected answer length: &le; 8 lines).\n",
    "</b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>[QUESTION 8] Answer</b>  <br>\n",
    "**Insert your answer here.**\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a4385-7edd-40c8-8ef8-935bfc9846d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: normalize the rows of your dataset, and train a k-NN classifier on the training set\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483853d-292a-49d3-a9b6-c869b1414a93",
   "metadata": {},
   "source": [
    "**Compute the accuracy** of your classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb241b51-302e-4044-a6a5-7e913922a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: compute the accuracy of the classifier\n",
    "accuracy = ...\n",
    "print(\"The accuracy of the model is \", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c4f44-274f-4964-b0c7-23c0ede882d8",
   "metadata": {},
   "source": [
    "**Visualize the train and test pixel locations on the ground truth map**. Color the testing pixels in green, the training pixels in red and the background in white. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9cfb2f-d364-45b3-8cc3-d9d9ca70032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: visualize the train/test pixel coordinates\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbbbd01-6fd0-4448-b1b7-5e20d2ef0681",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[QUESTION 9] Distribution of the training and testing</b>  <br>\n",
    "    \n",
    "- What do you notice about how the training and test pixels are distributed spatially?\n",
    "- Knowing that this model should generalize to other hyperspectral images in the future, is this sampling strategy good? Justify.\n",
    "\n",
    "(Expected answer length: &le; 5 lines).\n",
    "</b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>[QUESTION 9] Answer</b>  <br>\n",
    "**Insert your answer here.**\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394ae66-f59c-4922-8a26-235af14aaa78",
   "metadata": {},
   "source": [
    "# **Part III.3 - k-NN classification with block-based sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107cba0-80de-45ab-a2bf-b4db147c6183",
   "metadata": {},
   "source": [
    "This time, we will try a different approach. Instead of using the spectrum of each pixel, we will **divide the image into spatial blocks**, and use the center pixel of each block for classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9513592b-50dd-409e-acd0-33797064a3e3",
   "metadata": {},
   "source": [
    "Your tasks are the following:\n",
    "\n",
    "1. Extract patches of size 3x3 pixels. For each patch, only the center pixel is used for classification.\n",
    "2. Skip background pixels (where gt == 0).\n",
    "3. Collect the spectral vector of each center pixel, along with its label.\n",
    "4. Split the data into training (75%) and testing (25%) sets. Be careful: make sure that the class distribution is preserved in both subsets (i.e., each class should appear in similar proportions in the training and testing sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b14f57-d8fb-45b1-abb7-3b0bf5adba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: divide your dataset into training and testing sets, according to the methodology described above.\n",
    "X_train, y_train = ...\n",
    "X_test, y_test = ...qsdfqf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90be0f3a-32f8-473b-8541-2216b3a046d6",
   "metadata": {},
   "source": [
    "**Visualize the train and test pixel locations on the ground truth map**. Color the testing pixels in green, the training pixels in red and the background in white. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98c831f-7df0-49dc-96f8-fefe9f69c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: visualize the train/test pixel coordinates\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037cf51-5232-4cc9-82ea-350f6ce5d6a8",
   "metadata": {},
   "source": [
    "**Train a k-NN classifier** with `k = 5` and the `euclidean` distance. Before training it, you should **normalize** the rows of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb3a52-263b-4499-9855-c94c86223e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: normalize the rows of your dataset, and train a k-NN classifier on the training set\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec77195d-69c1-4374-a9ed-07020cbbe4d4",
   "metadata": {},
   "source": [
    "**Compute the accuracy** of your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2123c9cf-0c65-439e-8c91-7c75984c09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: compute the accuracy of the classifier\n",
    "accuracy = ...\n",
    "print(\"The accuracy of the model is \", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2a4ed-498f-4477-ae1b-ef2c7e1b4f7c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[QUESTION 10] Block-based sampling</b>  <br>\n",
    "Re-run your code using spatial blocks of 5×5 pixels, then 7×7.\n",
    "\n",
    "- How does the accuracy change as the block size increases?\n",
    "- Considering that the classifier should generalize to other hyperspectral images, do larger block sizes lead to better generalization?\n",
    "\n",
    "(Expected answer length: &le; 5 lines).\n",
    "</b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>[QUESTION 10] Answer</b>  <br>\n",
    "**Insert your answer here.**\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e62a44",
   "metadata": {},
   "source": [
    "# **Part IV - Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c18325",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[QUESTION 11] Conclusion</b>  <br>\n",
    "You must explain this project to an EPL student who has not taken the course using an illustration explaining the pre-processing to be implemented for these images (dimension management, RGB/RGB-A, etc.) and summarize one of the three parts (Satellite, dogs & cats or hyperspectral). You are free to create the illustration however you wish (as a slide or a diagram for example). It must only be included directly in this Jupyter notebook as an image or PDF file.\n",
    "</b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>[QUESTION 11] Answer</b>  <br>\n",
    "...\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ddf15",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
